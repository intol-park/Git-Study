{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "\u001b[1m263/263\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2ms/step - loss: 7.5194e-04\n",
      "Epoch 2/50\n",
      "\u001b[1m263/263\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 3.8706e-04\n",
      "Epoch 3/50\n",
      "\u001b[1m263/263\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 2.7699e-04\n",
      "Epoch 4/50\n",
      "\u001b[1m263/263\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 2.5300e-04\n",
      "Epoch 5/50\n",
      "\u001b[1m263/263\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 2.4702e-04\n",
      "Epoch 6/50\n",
      "\u001b[1m263/263\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 2.3373e-04\n",
      "Epoch 7/50\n",
      "\u001b[1m263/263\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 2.3332e-04\n",
      "Epoch 8/50\n",
      "\u001b[1m263/263\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 2.3369e-04\n",
      "Epoch 9/50\n",
      "\u001b[1m263/263\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 2.2975e-04\n",
      "Epoch 10/50\n",
      "\u001b[1m263/263\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 2.3602e-04\n",
      "Epoch 11/50\n",
      "\u001b[1m263/263\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 2.2437e-04\n",
      "Epoch 12/50\n",
      "\u001b[1m263/263\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 2.2300e-04\n",
      "Epoch 13/50\n",
      "\u001b[1m263/263\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 2.2495e-04\n",
      "Epoch 14/50\n",
      "\u001b[1m263/263\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 2.2508e-04\n",
      "Epoch 15/50\n",
      "\u001b[1m263/263\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 2.2548e-04\n",
      "Epoch 16/50\n",
      "\u001b[1m263/263\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 2.2392e-04\n",
      "Epoch 17/50\n",
      "\u001b[1m263/263\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 2.1880e-04\n",
      "Epoch 18/50\n",
      "\u001b[1m263/263\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 2.2572e-04\n",
      "Epoch 19/50\n",
      "\u001b[1m263/263\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 2.2204e-04\n",
      "Epoch 20/50\n",
      "\u001b[1m263/263\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 2.3161e-04\n",
      "Epoch 21/50\n",
      "\u001b[1m263/263\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 2.1811e-04\n",
      "Epoch 22/50\n",
      "\u001b[1m263/263\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 2.2358e-04\n",
      "Epoch 23/50\n",
      "\u001b[1m263/263\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 2.2519e-04\n",
      "Epoch 24/50\n",
      "\u001b[1m263/263\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 2.2414e-04\n",
      "Epoch 25/50\n",
      "\u001b[1m263/263\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 2.1810e-04\n",
      "Epoch 26/50\n",
      "\u001b[1m263/263\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 2.1581e-04\n",
      "Epoch 27/50\n",
      "\u001b[1m263/263\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 2.2750e-04\n",
      "Epoch 28/50\n",
      "\u001b[1m263/263\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 2.2993e-04\n",
      "Epoch 29/50\n",
      "\u001b[1m263/263\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 2.1569e-04\n",
      "Epoch 30/50\n",
      "\u001b[1m263/263\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 2.2029e-04\n",
      "Epoch 31/50\n",
      "\u001b[1m263/263\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 2.1966e-04\n",
      "Epoch 32/50\n",
      "\u001b[1m263/263\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 2.1939e-04\n",
      "Epoch 33/50\n",
      "\u001b[1m263/263\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 2.2530e-04\n",
      "Epoch 34/50\n",
      "\u001b[1m263/263\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 2.1488e-04\n",
      "Epoch 35/50\n",
      "\u001b[1m263/263\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 2.2259e-04\n",
      "Epoch 36/50\n",
      "\u001b[1m263/263\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 2.1398e-04\n",
      "Epoch 37/50\n",
      "\u001b[1m263/263\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 2.1622e-04\n",
      "Epoch 38/50\n",
      "\u001b[1m263/263\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 2.2194e-04\n",
      "Epoch 39/50\n",
      "\u001b[1m263/263\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 2.2511e-04\n",
      "Epoch 40/50\n",
      "\u001b[1m263/263\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 2.1892e-04\n",
      "Epoch 41/50\n",
      "\u001b[1m263/263\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 2.1791e-04\n",
      "Epoch 42/50\n",
      "\u001b[1m263/263\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 2.1759e-04\n",
      "Epoch 43/50\n",
      "\u001b[1m263/263\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 2.2178e-04\n",
      "Epoch 44/50\n",
      "\u001b[1m263/263\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 2.2249e-04\n",
      "Epoch 45/50\n",
      "\u001b[1m263/263\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 2.1869e-04\n",
      "Epoch 46/50\n",
      "\u001b[1m263/263\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 2.1956e-04\n",
      "Epoch 47/50\n",
      "\u001b[1m263/263\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 2.1762e-04\n",
      "Epoch 48/50\n",
      "\u001b[1m263/263\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 2.2154e-04\n",
      "Epoch 49/50\n",
      "\u001b[1m263/263\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 2.2098e-04\n",
      "Epoch 50/50\n",
      "\u001b[1m263/263\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 2.1419e-04\n",
      "Training completed.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Input, Dense, Subtract\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "\n",
    "# Define the base network\n",
    "def create_base_network(input_dim):\n",
    "    input = Input(shape=(input_dim,))\n",
    "    x = Dense(64, activation='relu')(input)\n",
    "    x = Dense(32, activation='relu')(x)\n",
    "    x = Dense(16, activation='relu')(x)\n",
    "    output = Dense(1, activation='linear')(x)  # Output layer for regression\n",
    "    return Model(input, output)\n",
    "\n",
    "# Define the twin neural network model\n",
    "def create_twin_network(input_dim):\n",
    "    base_network = create_base_network(input_dim)\n",
    "    \n",
    "    input_a = Input(shape=(input_dim,))\n",
    "    input_b = Input(shape=(input_dim,))\n",
    "    \n",
    "    processed_a = base_network(input_a)\n",
    "    processed_b = base_network(input_b)\n",
    "    \n",
    "    # Compute the difference between the outputs\n",
    "    diff = Subtract()([processed_a, processed_b])\n",
    "    \n",
    "    return Model([input_a, input_b], diff)\n",
    "\n",
    "# Custom loss function for semi-supervised learning\n",
    "def custom_loss_function(lambda_value, b):\n",
    "    def loss(y_true, y_pred):\n",
    "        labeled_mask = tf.cast(tf.not_equal(y_true, -1), tf.float32)\n",
    "        unlabeled_mask = 1.0 - labeled_mask\n",
    "        \n",
    "        # Supervised loss for labeled data\n",
    "        supervised_loss = (2 / b) * tf.reduce_sum(\n",
    "            tf.square(y_pred - y_true) * labeled_mask\n",
    "        )\n",
    "        \n",
    "        # Unsupervised loss for loop consistency\n",
    "        unsupervised_loss = (2 * lambda_value / b) * tf.reduce_sum(\n",
    "            tf.square(y_pred) * unlabeled_mask\n",
    "        )\n",
    "        \n",
    "        return supervised_loss + unsupervised_loss\n",
    "    \n",
    "    return loss\n",
    "\n",
    "# Create the dataset (dummy data for illustration)\n",
    "num_samples = 100\n",
    "input_dim = 10\n",
    "X_train = np.random.rand(num_samples, input_dim)\n",
    "y_train = np.random.rand(num_samples)\n",
    "\n",
    "# Create labeled pairs for training\n",
    "labeled_pairs = []\n",
    "labeled_labels = []\n",
    "\n",
    "for i in range(num_samples):\n",
    "    for j in range(i + 1, num_samples):\n",
    "        labeled_pairs.append([X_train[i], X_train[j]])\n",
    "        labeled_labels.append(y_train[i] - y_train[j])\n",
    "\n",
    "labeled_pairs = np.array(labeled_pairs)\n",
    "labeled_labels = np.array(labeled_labels)\n",
    "\n",
    "# Add unlabeled pairs (use -1 as a marker for unlabeled data)\n",
    "num_unlabeled = 30\n",
    "unlabeled_data = np.random.rand(num_unlabeled, input_dim)\n",
    "unlabeled_pairs = []\n",
    "unlabeled_labels = []\n",
    "\n",
    "for i in range(num_unlabeled):\n",
    "    for j in range(i + 1, num_unlabeled):\n",
    "        unlabeled_pairs.append([unlabeled_data[i], unlabeled_data[j]])\n",
    "        unlabeled_labels.append(-1)  # -1 indicates unlabeled data\n",
    "\n",
    "unlabeled_pairs = np.array(unlabeled_pairs)\n",
    "unlabeled_labels = np.array(unlabeled_labels)\n",
    "\n",
    "# Add cross pairs between labeled and unlabeled data\n",
    "cross_pairs = []\n",
    "cross_labels = []\n",
    "\n",
    "for i in range(num_samples):\n",
    "    for j in range(num_unlabeled):\n",
    "        cross_pairs.append([X_train[i], unlabeled_data[j]])\n",
    "        cross_labels.append(-1)  # -1 indicates semi-supervised pair\n",
    "\n",
    "cross_pairs = np.array(cross_pairs)\n",
    "cross_labels = np.array(cross_labels)\n",
    "\n",
    "# Combine all pairs\n",
    "all_pairs = np.concatenate([labeled_pairs, unlabeled_pairs, cross_pairs])\n",
    "all_labels = np.concatenate([labeled_labels, unlabeled_labels, cross_labels])\n",
    "\n",
    "# Define and compile the model\n",
    "twin_network = create_twin_network(input_dim)\n",
    "twin_network.compile(optimizer=Adam(learning_rate=0.001), loss=custom_loss_function(lambda_value=1.0, b=len(all_pairs)))\n",
    "\n",
    "# Train the model\n",
    "twin_network.fit([all_pairs[:, 0], all_pairs[:, 1]], all_labels, epochs=50, batch_size=32)\n",
    "\n",
    "print(\"Training completed.\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
